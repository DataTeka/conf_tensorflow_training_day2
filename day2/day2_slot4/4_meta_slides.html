<!DOCTYPE html>
<html>
  <head>
    <title>Representation learning, interpretability, and uncertainty</title>
    <meta charset="utf-8">
    <meta name="author" content="Sigrid Keydana" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Representation learning, interpretability, and uncertainty
## A pick of ‘meta’ topics in current DL
### Sigrid Keydana
### rstudio::conf 2019

---








# Representation learning, interpretability, and uncertainty

What do these topics have in common?

--

- Terms are not necessarily well-defined

- No out-of-the-box solutions

- You will have to evaluate what's useful _to you_

- We just present techniques to choose from / adapt / extend ...

&lt;br /&gt;

--

The idea being: Enable you to apply what's useful to you _in your area of work/interest_


---
class: inverse, middle, center

# Representation learning


---

# Representation matters.


Your task: Separate the green triangles from the blue circles.&lt;sup&gt;1&lt;/sup&gt;


![](images/representationmatters.png)




.footnote[[1] Goodfellow, I, Y. Bengio and A. Courville (2016). Deep Learning. http://www.deeplearningbook.org.]

---

# What makes one representation better than another?

Good representations make a subsequent prediction task easier.&lt;sup&gt;1, 2&lt;/sup&gt;

Examples: 

- unsupervised pretraining (make use of vast amounts of unlabeled data)
- transfer learning (use features learned on one task for solving another task)
- use relations learned on one domain to infer relationships on another




.footnote[[1] See chapter 15 in Goodfellow, I, Y. Bengio and A. Courville (2016). Deep Learning. http://www.deeplearningbook.org.

[2] Also, Bengio, Y, A. C. Courville and P. Vincent (2012). Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives. http://arxiv.org/abs/1206.5538.]

---
# Example: Zero-shot learning with DeVise

- Frome et al.&lt;sup&gt;1&lt;/sup&gt; trained an image classification model to predict label embeddings (that had been learned in an supervised manner) instead of the labels themselves

- This allows for meaningful classification of objects not encountered before

![](images/devise.png)



.footnote[[1] Frome, A., G. S. Corrado, J. Shlens, et al. (2013). DeViSE: A Deep Visual-Semantic Embedding Model]


---
# To allow for good generalization, good latent features are

- causal

- disentangled

- smooth

- possibly, hierarchical

- shared across domains / tasks

- sparse 

---
# Example: Disentanglement

- Radford et al.&lt;sup&gt;1&lt;/sup&gt; showed they could perform word-vector-like arithmetic manipulations on the space larned by a DCGAN (deep generative adversarial network):

![](images/glasses_scaled.png)



.footnote[[1] Radford, A, L. Metz and S. Chintala (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks]

---
# Other uses of representation learning

Besides improving prediction, latent spaces (resp. learned distributions) also figure prominently in:

- object generation

- evaluating data likelihood at test time

- learning conditional relationships

--

&lt;br /&gt;

Let's see an example!

---

## Variational autoencoders (VAEs)


- Density estimation
- Object generation

- often used in object generation
- learn a _latent space_ that can be sampled from
- learning objective is a tradeoff between faithful input reconstruction and conforming to a (regularizing) prior
- Minimize: Evidence lower bound (ELBO)

`$$ELBO\ = \ E[log\ p(x|z)]\ -\ KL(q(z)||p(z))$$`

--

#### Problems with traditional VAEs

- may learn uninformative latent space
- tends to overfit the data

---

# InfoVAE and subtypes

- designed to learn more meaningful features
- e.g. by maximizing mutual information between latent space prior and posterior
- we'll see a subtype (MMD-VAE) soon

--

But before, we need some background on

---
class: inverse, middle, center

# Eager execution


---

# Eager execution

- The non-graph way of doing TensorFlow/Keras
- More intuitive than static Keras/TF especially if not following the usual define-compile-fit cycle:
  - encoder-decoder models (as in GANs, VAEs ...)
  - neural style transfer
  - models with attention mechanisms
  
--

#### Detailed examples on the TensorFlow for R blog:

- [More flexible models with TensorFlow eager execution and Keras](https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup/)
- [Image-to-image translation with pix2pix](https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix/)
- [Attention-based Image Captioning with Keras](https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/)
- [Neural style transfer with eager execution and Keras](https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer/)
- [Generating images with Keras and TensorFlow eager execution](https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/)
- [Attention-based Neural Machine Translation with Keras](https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/)


---

# Eager execution exercise

- demo: [eager_intro.Rmd](../notebooks/eager_intro.Rmd)
- exercise: [4_meta_quizzes.Rmd](../exercises/4_meta_quizzes.Rmd)


---
class: inverse, middle, center

# Variational autoencoder (MMD-VAE)


---

# Representation learning with MMD-VAE

- Idea described in _InfoVAE: Information Maximizing Variational Autoencoders_&lt;sup&gt;1&lt;/sup&gt;

- Read up later here: [Representation learning with MMD-VAE](https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae)

- Complete code here: [mmd_cvae.R](https://github.com/rstudio/keras/blob/master/vignettes/examples/mmd_cvae.R)

- We'll take a bird's view here, not going into every detail



.footnote[[1] cf. Zhao, Song, and Ermon (2017)]

---
# MMD-VAE, main steps (1): data streaming


```r
library(keras)
use_implementation("tensorflow")
library(tensorflow)
tfe_enable_eager_execution(device_policy = "silent")

library(tfdatasets)
# also load dplyr, ggplot2, glue

fashion &lt;- dataset_fashion_mnist()
c(train_images, train_labels) %&lt;-% fashion$train
c(test_images, test_labels) %&lt;-% fashion$test

train_x &lt;- train_images %&gt;% `/`(255) %&gt;%
  k_reshape(c(60000, 28, 28, 1))

test_x &lt;- test_images %&gt;% `/`(255) %&gt;%
  k_reshape(c(10000, 28, 28, 1))

buffer_size &lt;- 60000
batch_size &lt;- 100

*train_dataset &lt;- tensor_slices_dataset(train_x) %&gt;%
*  dataset_shuffle(buffer_size) %&gt;%
*  dataset_batch(batch_size)
```

---
# MMD-VAE, main steps (2): encoder model


```r
encoder_model &lt;- function(name = NULL) {
  keras_model_custom(name = name, function(self) {
*    self$conv1 &lt;- 
*      layer_conv_2d(             # actually a convolutional encoder
        filters = 32,
        kernel_size = 3,
        strides = 2,
        activation = "relu")
     self$conv2 &lt;-
      layer_conv_2d(
        filters = 64,
        kernel_size = 3,
        strides = 2,
        activation = "relu")
    self$flatten &lt;- layer_flatten()
*   self$dense &lt;- layer_dense(units = latent_dim)     # 2 for easy viz
    function (x, mask = NULL) {
      x %&gt;%
        self$conv1() %&gt;%
        self$conv2() %&gt;%
        self$flatten() %&gt;%
        self$dense()
      }})}
```


---
# MMD-VAE, main steps (3): decoder model



```r
decoder_model &lt;- function(name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    self$dense &lt;- layer_dense(units = 7 * 7 * 32, activation = "relu")
    self$reshape &lt;- layer_reshape(target_shape = c(7, 7, 32))
*    self$deconv1 &lt;- # first of 3 up-convolutions
      layer_conv_2d_transpose(
        filters = 64,
        kernel_size = 3,
        strides = 2,
        padding = "same",
        activation = "relu"
      )
*  self$deconv2 &lt;-  # another conv_2d_transpose, filters=32
*  self$deconv3 &lt;-  # another conv_2d_transpose, filters=1
    function (x, mask = NULL) {
      x %&gt;%
        self$dense() %&gt;%
        self$reshape() %&gt;%
        self$deconv1() %&gt;%
        self$deconv2() %&gt;%
        self$deconv3()
    }
  })}
```

---
# MMD-VAE, main steps (4): MMD loss

The loss, _maximum mean discrepancy_ (MMD), is based on the idea that two distributions are identical if and only if all moments are identical. 

Concretely, MMD is estimated using a _kernel_, such as the Gaussian kernel

`$$k(z,z')=\frac{e^{||z-z'||}}{2\sigma^2}$$`

to assess similarity between distributions.

The idea then is that if two distributions are identical, the average similarity between samples from each distribution should be identical to the average similarity between mixed samples from both distributions:

`$$MMD(p(z)||q(z))=E_{p(z),p(z')}[k(z,z')]+E_{q(z),q(z')}[k(z,z')]−2E_{p(z),q(z')}[k(z,z')]$$`

---
# MMD-VAE, main steps (4): MMD loss



```r
compute_kernel &lt;- function(x, y) {
  x_size &lt;- k_shape(x)[1]
  y_size &lt;- k_shape(y)[1]
  dim &lt;- k_shape(x)[2]
  tiled_x &lt;- k_tile(
    k_reshape(x, k_stack(list(x_size, 1, dim))),
    k_stack(list(1, y_size, 1))
  )
  tiled_y &lt;- k_tile(
    k_reshape(y, k_stack(list(1, y_size, dim))),
    k_stack(list(x_size, 1, 1))
  )
  k_exp(-k_mean(k_square(tiled_x - tiled_y), axis = 3) /
          k_cast(dim, tf$float64))
}

compute_mmd &lt;- function(x, y, sigma_sqr = 1) {
  x_kernel &lt;- compute_kernel(x, x)
  y_kernel &lt;- compute_kernel(y, y)
  xy_kernel &lt;- compute_kernel(x, y)
  k_mean(x_kernel) + k_mean(y_kernel) - 2 * k_mean(xy_kernel)
}
```


---
# MMD-VAE, main steps (5): training loop


```r
for (epoch in seq_len(num_epochs)) {
  iter &lt;- make_iterator_one_shot(train_dataset)
  until_out_of_range({
    x &lt;-  iterator_get_next(iter)
    with(tf$GradientTape(persistent = TRUE) %as% tape, {
      mean &lt;- encoder(x)
      preds &lt;- decoder(mean)
      true_samples &lt;- k_random_normal(
        shape = c(batch_size, latent_dim), dtype = tf$float64)
*    loss_mmd &lt;- compute_mmd(true_samples, mean)
*      loss_nll &lt;- k_mean(k_square(x - preds))
*      loss &lt;- loss_nll + loss_mmd
    })
    encoder_gradients &lt;- tape$gradient(loss, encoder$variables)
    decoder_gradients &lt;- tape$gradient(loss, decoder$variables)
    optimizer$apply_gradients(purrr::transpose(list(
      encoder_gradients, encoder$variables
    )),
    global_step = tf$train$get_or_create_global_step())
    optimizer$apply_gradients(purrr::transpose(list(
      decoder_gradients, decoder$variables
    )),
    global_step = tf$train$get_or_create_global_step())
  })}
```


---
# MMD-VAE: Generated images

![](images/mmd_clothes_epoch_50.png)


---
# MMD-VAE: Latent space

![](images/mmd_latentspace_epoch_50.png)

---
# MMD-VAE: Interpolations

![](images/mmd_grid_epoch_50.png)


---
# Variational autoencoders: Exercise

- exercise: [4_meta_quizzes.Rmd](../exercises/4_meta_quizzes.Rmd)


---
class: inverse, middle, center

# Interpretability

---
# Interpretability/explainability

- Everybody wants it but it's actually not a well-defined term
- Commonly, a division is made between _interpreting a model_ and _explaining model decisions_
- But see e.g. "The Mythos of Model Interpretability"&lt;sup&gt;1&lt;/sup&gt; for the lack of agreed-upon definitions
- Here, we'll skip any theoretical discussion and focus on two different ways of _local explanations_
- Just one quote on the theory before we leave that be&lt;sup&gt;2&lt;/sup&gt;

&gt; However, it is
fair to say that most work in explainable artificial intelligence uses only the researchers’
intuition of what constitutes a ‘good’ explanation.  There exists vast and valuable bodies
of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain
cognitive  biases  and  social  expectations  towards  the  explanation  process. 




.footnote[[1] cf. Lipton (2016)

[2] cf. Miller (2017)]

---
# LIME&lt;sup&gt;1&lt;/sup&gt;

.pull-left[![](images/lime1.png)]
.pull-right[![](images/lime2.png)]




.footnote[[1] cf. Ribeiro, Singh, and Guestrin (2016)]


---
# LIME on images

- Uses obfuscation on differently-sized _superpixels_
- As LIME works by switching presence of superpixels on and off, obtaining an explanation for a high-res image can take some time
- We will thus resize images before passing them to LIME as anyway the model will work with that size


---
# LIME (1): Create an explainer

##### We first create an _explainer_ using the `lime` factory function


```r
# the first argument is the path to the image
# the second argument should indicate to LIME the type of model
# the third argument is a preprocessing function we need to define

explainer &lt;- lime(img_path, as_classifier(model, labels), image_prep)
```


##### And here's the preprocessing function that has to be passed to `lime`


```r
image_prep &lt;- function(x) {
  arrays &lt;- lapply(x, function(path) {
    img &lt;- image_load(path, target_size = c(224,224)) %&gt;%
      image_to_array()
    img &lt;- img %&gt;%
      array_reshape(c(1, dim(img))) %&gt;%
      imagenet_preprocess_input()
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}
```


---
# LIME(2): Get a prediction

##### This also allows to test the preprocessing function


```r
preds &lt;- predict(model, image_prep(img_path))
```

##### Decode prediction


```r
preds %&gt;% imagenet_decode_predictions()
```

```
  class_name   class_description      score
1  n02782093           balloon    9.999470e-01
2  n03888257         parachute    3.710761e-05
3  n02692877           airship    1.597529e-05
4  n04562935       water_tower    4.031146e-08
5  n04507155          umbrella    2.227399e-08
```

---
# LIME (3): Explore superpixels

##### Explore different settings for number of superpixels before actually getting an explanation:


```r
plot_superpixels(img_path, colour = "cyan",
                 n_superpixels = 50, weight = 10)
plot_superpixels(img_path, colour = "cyan",
                 n_superpixels = 200, weight = 10)
```

![Using 50 vs. 200 superpixels](images/superpixels.png)


---
# LIME (4): Get explanation

##### Call `explain` with the path to the image, the `explainer`, and optional parameters:


```r
explanation &lt;- explain(
  img_path, 
  explainer,
  n_labels = 1, 
  n_features = 200,
  n_superpixels = 200,
  weight = 20)
```


```r
plot_image_explanation(explanation, fill_alpha = 0.6)
plot_image_explanation(explanation, display = "block", block_col = "violet")
```

![2 of several ways to display an explanation](images/explanations.png)


---
# Class activation maps (CAM)

- For a given input image, class activation maps take the __output feature maps__ of a convolution layer and __weigh__ every channel in that feature map by the __gradient of the most probable class__ with respect to the channel.

- Meaning: How much would that class probability change if that channel's activations were to change?


---
# CAM (1): Get the most probable class


```r
preds &lt;- model %&gt;% predict(img)
max_pred &lt;- which.max(preds[1,])

image_output &lt;- model$output[, max_pred]
```

We will want to compute this class output's gradient with respect to the activations of all channels in the last conv layer.

---
# CAM (2): Get the last conv layer's output


```r
last_conv_layer &lt;- model %&gt;% get_layer("block5_conv3")
last_conv_layer_output &lt;- last_conv_layer$output
```

These are the activations w.r.t. which we want to compute the class probability's gradient.

---
# CAM (3): Fetch the pooled gradients


We build a little graph to fetch and average the gradients - and run it.


```r
grads &lt;- k_gradients(image_output, last_conv_layer_output)[[1]]
# average gradients over all but the channel dimension
pooled_grads &lt;- k_mean(grads, axis = c(1, 2, 3))

iterate &lt;- k_function(
  list(model$input),
  list(pooled_grads, last_conv_layer_output[1, , , ])
)

c(pooled_grads_value, conv_layer_output_value) %&lt;-% iterate(list(img))
```


---
# CAM (4): Multiply outputs by channel gradients

This weights up the channels that have high impact on the classification.


```r
for (i in 1:512) {
  conv_layer_output_value[ , , i] &lt;- 
    conv_layer_output_value[ , , i] * pooled_grads_value[i] 
}
```

Averaging over the channels dimension yields the heatmap:



```r
heatmap &lt;- apply(conv_layer_output_value, c(1,2), mean)
```

---
# CAM (5): Heatmap (overlayed over image)

![](images/cam.png)


---
# LIME/CAM: Exercise

- demo: [lime.Rmd](../notebooks/lime.Rmd) (not needed for this exercise)
- demo: [class_activation_maps.Rmd](../notebooks/class_activation_maps.Rmd) (not needed for this exercise)
- exercise: [4_meta_quizzes.Rmd](../exercises/4_meta_quizzes.Rmd)


---
class: inverse, middle, center
# Obtaining uncertainty estimates from neural networks

---
# Why uncertainty estimates?

- If we can _quantify_ network uncertainty, we can take appropriate measures (like, have humans inspect uncertain cases)&lt;sup&gt;1&lt;/sup&gt;
- If we can _qualify_ network uncertainty (as in, e.g., reducible vs. irreducible), we can take appropriate counteractions (e.g., factoring in error margins)

![Figure from Leibig et al.](images/retinopathy_scaled.png)

- However, this is uncharted territory _to a higher degree even_  than interpretability (where we've just seen two out of quite a few available approaches)
- There is one rather established approach for this though 





.footnote[[1] cf. Leibig, Allken, Seckin Ayhan, et al. (2017)]

---
# Uncertainty estimates through Bayesian deep learning

- Apply dropout at test time (Gal &amp; Gharahmani 2016&lt;sup&gt;1&lt;/sup&gt;)
- Have the network learn the dropout rate (Gal et al. 2017&lt;sup&gt;2&lt;/sup&gt;, Kendall &amp; Gal 2017&lt;sup&gt;3&lt;/sup&gt;)
- We are going to look at the latter approach




.footnote[
[1] cf. Gal and Ghahramani (2016)

[2] cf. Gal, Hron, and Kendall (2017)

[3] cf. Kendall and Gal (2017)
]

---
# Types of uncertainty

- Epistemic: What the model doesn't know (reducible by more data)
- Aleatoric: Variations due to sampling/measurement process (irreducible)


Example from class segmentation:&lt;sup&gt;1&lt;/sup&gt;

![](images/segmentation_uncertainty_scaled.png)



.footnote[
[1] cf. Kendall and Gal (2017)]


---
# Implementation in R

- Implementation follows the authors' Python Keras version
- Read up later here: [You sure? A Bayesian approach to obtaining uncertainty estimates from neural networks](https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout/)
- Consists of 4 parts: a layer wrapper, a modified model, a specific loss function, and Monte Carlo prediction

---
# Wrapper class (extract)

The wrapper's purpose is to calculate the optimal dropout rate for the layer it wraps.


```r
# R6 wrapper class, a subclass of KerasWrapper
ConcreteDropout &lt;- R6::R6Class("ConcreteDropout",
  
  inherit = KerasWrapper,
  
  public = list(
    # [...]
    initialize = function( # [...]
    
    build = function(input_shape) {
      super$build(input_shape)
*    self$p_logit &lt;- super$add_weight(# [...]
      # [...]
      regularizer &lt;- k_sum(kernel_regularizer + dropout_regularizer)
*    super$add_loss(regularizer)
    },
    
    concrete_dropout = function(x) { # [...]
      
    call = function(x, mask = NULL, training = NULL) {# [...]
  )
)
```


---
# Dropout model (1)

This example has 3 hidden dense layers each wrapped with `ConcreteDropout`.


```r
output &lt;- input %&gt;% layer_concrete_dropout(
  layer = layer_dense(units = hidden_dim, activation = "relu"),
  # [...]
  ) %&gt;% layer_concrete_dropout(
  layer = layer_dense(units = hidden_dim, activation = "relu"),
  # [...]
  ) %&gt;% layer_concrete_dropout(
  layer = layer_dense(units = hidden_dim, activation = "relu"),
  # [...]
)
```

---
# Dropout model (2)

The model outputs not just the _predictive (conditional) mean_, but also the _predictive variance_:


```r
mean &lt;- output %&gt;% layer_concrete_dropout(
  layer = layer_dense(units = output_dim),
  # [...]
)

log_var &lt;- output %&gt;% layer_concrete_dropout(
  layer_dense(units = output_dim),
  # [...]
)
output &lt;- layer_concatenate(list(mean, log_var))
model &lt;- keras_model(input, output)
```

- This means we can learn _different variances for different data points_. 
- We thus hope to be able to account for _heteroscedasticity_ in the data.

---
# Heteroscedastic loss (1)

Instead of mean squared error we use a cost function that does not treat all estimates alike:

`$$\frac{1}{N} \sum_i{\frac{1}{2 \hat{\sigma}^2_i} \ (\mathbf{y}_i - \mathbf{\hat{y}}_i)^2 + \frac{1}{2} log \ \hat{\sigma}^2_i}$$`

In addition to the target vs. prediction check, this cost function contains two regularization terms: 

- `\(\frac{1}{2 \hat{\sigma}^2_i}\)` downweights the high-uncertainty predictions in the loss function. The model is encouraged to indicate high uncertainty when its predictions are false.

- `\(\frac{1}{2} log \ \hat{\sigma}^2_i\)` makes sure the network does not simply indicate high uncertainty everywhere.

---
# Heteroscedastic loss (2)

R code (calculating with the log of the variance):


```r
heteroscedastic_loss &lt;- function(y_true, y_pred) {
    mean &lt;- y_pred[, 1:output_dim]
    log_var &lt;- y_pred[, (output_dim + 1):-1]
    precision &lt;- k_exp(-log_var)
    k_sum(precision * (y_true - mean) ^ 2 + log_var, axis = 2)
}

model %&gt;% compile(
  optimizer = "adam",
  loss = heteroscedastic_loss,
  metrics = c(custom_metric("heteroscedastic_loss", heteroscedastic_loss))
)
```


---
# Obtain uncertainty estimates (Monte Carlo)


```r
MC_samples &lt;- array(0, dim = c(num_MC_samples, n_val, 2 * output_dim))
for (k in 1:num_MC_samples) {
  MC_samples[k, , ] &lt;- (model %&gt;% predict(X_val))
}
```

Epistemic uncertainty is calculated from the model's "mean" output:


```r
means &lt;- MC_samples[, , 1:output_dim] 
# predictive mean (average over the MC samples)
predictive_mean &lt;- apply(means, 2, mean) 
# epistemic_uncertainty = variance of the "mean" component of the MC samples
epistemic_uncertainty &lt;- apply(means, 2, var) 
```


While aleatoric uncertainty is calculated from the "variance" output:


```r
# aleatoric_uncertainty = mean of the "variance" component of the MC samples
logvar &lt;- MC_samples[, , (output_dim + 1):dim(MC_samples)[3]]
aleatoric_uncertainty &lt;- exp(colMeans(logvar))
```

---
# Results: Epistemic uncertainty

Epistemic uncertainty on validation set (simulated data, x is Gaussian distributed, _train_n_ = 1000).

![](images/epistemic_1000.png)

Uncertainty is high in regions where the model has not seen many datapoints.

---
# Results: Aleatoric uncertainty

Aleatoric uncertainty on validation set (simulated data, x is Gaussian distributed, _train_n_ = 1000).

![](images/aleatoric_1000.png)

The amount of data seen does not affect irreducible uncertainty.

---
# Results: Predictive uncertainty

Overall predictive uncertainty on validation set (simulated data, x is Gaussian distributed, _train_n_ = 1000).

![](images/overall_1000.png)

Both types of uncertainty add to yield the final predictive uncertainty.


---
# Wrapup/feedback



---
# References

Bengio, Y, A. C. Courville and P. Vincent (2012). "Unsupervised
Feature Learning and Deep Learning: A Review and New
Perspectives". In: _CoRR_ abs/1206.5538. eprint: 1206.5538. URL:
[http://arxiv.org/abs/1206.5538](http://arxiv.org/abs/1206.5538).

Frome, A., G. S. Corrado, J. Shlens, et al. (2013). "DeViSE: A
Deep Visual-Semantic Embedding Model". In: _NIPS_. , pp.
2121-2129.

Gal, Y. and Z. Ghahramani (2016). "Dropout as a Bayesian
Approximation: Representing Model Uncertainty in Deep Learning".
In: _Proceedings of the 33nd International Conference on Machine
Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_. ,
pp. 1050-1059. URL:
[http://jmlr.org/proceedings/papers/v48/gal16.html](http://jmlr.org/proceedings/papers/v48/gal16.html).

Gal, Y, J. Hron and A. Kendall (2017). "Concrete Dropout". In:
_ArXiv e-prints_. arXiv: 1705.07832 [stat.ML].

Goodfellow, I, Y. Bengio and A. Courville (2016). _Deep Learning_.
MIT Press. URL:
[http://www.deeplearningbook.org](http://www.deeplearningbook.org).

---
# References (cont.)

Kendall, A. and Y. Gal (2017). "What Uncertainties Do We Need in
Bayesian Deep Learning for Computer Vision?" In: _Advances in
Neural Information Processing Systems 30_. Ed. by I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan and R.
Garnett. Curran Associates, Inc, pp. 5574-5584. URL:
[http://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision.pdf](http://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision.pdf).

Leibig, C, V. Allken, M. Seckin Ayhan, et al. (2017). "Leveraging
uncertainty information from deep neural networks for disease
detection". In: _bioRxiv_. DOI:
[10.1101/084210](https://doi.org/10.1101/084210). eprint:
https://www.biorxiv.org/content/early/2017/10/18/084210.full.pdf.
URL:
[https://www.biorxiv.org/content/early/2017/10/18/084210](https://www.biorxiv.org/content/early/2017/10/18/084210).

Lipton, Z. C. (2016). "The Mythos of Model Interpretability". In:
_CoRR_ abs/1606.03490. eprint: 1606.03490. URL:
[http://arxiv.org/abs/1606.03490](http://arxiv.org/abs/1606.03490).

Miller, T. (2017). "Explanation in Artificial Intelligence:
Insights from the Social Sciences". In: _CoRR_ abs/1706.07269.
eprint: 1706.07269. URL:
[http://arxiv.org/abs/1706.07269](http://arxiv.org/abs/1706.07269).

Radford, A, L. Metz and S. Chintala (2015). "Unsupervised
Representation Learning with Deep Convolutional Generative
Adversarial Networks". In: _CoRR_ abs/1511.06434.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
