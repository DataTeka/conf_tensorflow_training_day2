---
title: "Working with a heterogeneous dataset (census income dataset)"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---
  
  
```{r}
library(keras)
library(readr)
library(dplyr)
library(ggplot2)
library(purrr)
```


## The task

Here, we're using the "Census Income" (a.k.a. "Adult") dataset available at the [UCI Machine Learning Repository](http://mlr.cs.umass.edu/ml/datasets/Census+Income).

We are going to predict binarized salary (< resp. > 50k).

The focus of this exercise is to experiment with different ways of handling the presence of continuous as well as categorical variables.
We will explore 3 ways to do this.

## Prepare the data

The data is available in the file `day2/1_structured_data/data/adult.data`.
Please read in the data and familiarize yourself with the dataset.
Short descriptions are contained in `day2/1_structured_data/data/adult.names`.


```{r}
#
```


The dataset contains missing values. For our current purpose it's okay to just remove all incomplete rows.

```{r}
#
```


For the dependent variable, the action to take is the same for all 3 approaches.
We transform the values to 0s resp. 1s.

```{r}
#
```

Check that the reported class imbalance (~ 1:3) really is present in the dataset.

```{r}
#
```


For the independent variables, there are some common actions we can take now, too.
First, if you haven't yet, convert the character variables into factors.

```{r}
#
```


Isolate the continuous variables into a new dataset, e.g. `x_train_continuous`.

```{r}
#
```

Scale the variables to a common scale so the NN can handle them well.
As `x_train_continuous` will be passed in to `fit` later, it should finally be converted to a matrix.

```{r}
#
```

Now also isolate the categorical variables into a subset, e.g., `x_train_categorical`. 

```{r}
#
```



## Way 1

In the first approach, we one-hot-encode all categorical variables. You can use keras `to_categorical` for that. 
Encode every variable separately.

```{r}

```


Now you can bind all columns (continuous and one-hot-encoded categorical ones) together into a train matrix `x_train_all`.
Expected dimensions are (batch_size, 113).

```{r}
```


Now create a fully connected model to classify the rows.

```{r}

```


... and train it for 20 epochs. Use something like `validation_split` to check for overfitting.

```{r}

```

What's the final accuracy on the validation split?


## Way 2

Now let's try a different way. We will have a very similar model using only dense layers, but this time we'll split the train data into 9 different inputs (one continuous and 8 categorical).
The idea is to give the network the chance to get some special knowledge on each input.

```{r}
input_continuous <- layer_input(shape = dim(x_train_continuous)[2]) 
input_workclass <- layer_input(shape = dim(workclass)[2])
input_education <- layer_input(shape = dim(education)[2])
input_marital_status <- layer_input(shape = dim(marital_status)[2])
input_relationship <- layer_input(shape = dim(relationship)[2])
input_occupation <- layer_input(shape = dim(occupation)[2])
input_race <- layer_input(shape = dim(race)[2])
input_sex <- layer_input(shape = dim(sex)[2])
input_native_country <- layer_input(shape = dim(native_country)[2])

inputs <- list(input_continuous, input_workclass, input_education, input_marital_status,
               input_relationship, input_occupation, input_race, input_sex, input_native_country)
```


You could for example connect 2 dense layers to every input, then unite all branches, add further dense layers and then, add the final classification layer.


```{r}
#
```


Now train the model and compare accuracies.

```{r}
#
```


## Way 3

Way 3 is similar to way 2 but now, we're embedding the different inputs (apart from the continous input which stays wired to a dense layer).

```{r}
input_continuous <- layer_input(shape = dim(x_train_continuous)[2]) 
input_workclass <- layer_input(shape = 1)
input_education <- layer_input(shape = 1)
input_marital_status <- layer_input(shape = 1)
input_occupation <- layer_input(shape = 1)
input_relationship <- layer_input(shape = 1)
input_race <- layer_input(shape = 1)
input_sex <- layer_input(shape = 1)
input_native_country <- layer_input(shape = 1)

inputs <- list(input_continuous, input_workclass, input_education, input_marital_status,
               input_occupation, input_relationship, input_race, input_sex, input_native_country)
```


Here, you will want to have an embedding layer wired to every input but the first; then flatten the layers and have another dense layer attached.

```{r}
#
```

Again train the model and check accuracy.

```{r}

```

How do accuracies compare?
